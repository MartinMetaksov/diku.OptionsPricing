\chapter{Related work}
\label{chapter:relatedwork}

In comparison with CPU-oriented architectures, modern many-core hardware offers much higher theoretical peak performance, but they also require significantly more effort and expertise in order to unlock this power.   It follows that a rich body of work was aimed at porting and optimizing real-world applications for the many-core architectures. This chapter reviews several related research directions and is structured as follows: Section~\ref{relwork-lib} reviews work aimed at accelerating various important algorithms by means of libraries, and concludes that an accelerated implementation of trinomial pricing is, to our knowledge, not publicly available. Section~\ref{relwork-lang} surveys several data-parallel languages and argues that none of them can derive the two efficient code versions presented in this thesis. Finally, Sections~\ref{relwork-analysis} recounts several static, dynamic and hybrid analysis techniques, and points out the ones that have inspired our implementations.

\section{Library Solutions}
\label{relwork-lib}

Library solutions have been aimed at providing efficient implementations for common-used algorithms. For example, Nvidia provides {\sc cudnn} for accelerating deep learning algorithms~\cite{cudnn}, and more recently, Mathworks and Nvidia are working together to accelerate Matlab libraries, resulting in the Matlab Parallel Computing Toolbox\footnote{More information can be found on https://www.mathworks.com/products/parallel-computing/features.html}. The toolbox provides high-level constructs, parallel for-loops, special array types and parallelized numerical algorithms.   

Furthermore, a more related body of work has been carried out on investigating the acceleration (in OpenCL) of risk modelling and derivative pricing using Monte-Carlo simulations~\cite{FinPar:TACO,LexiFiPricing}. The findings of this studies have led to extending the Futhark language with new constructs and compiler analysis, which are useful and applicable in general (i.e., beyond the scope of the studied applications).

This thesis takes a similar approach, by providing a CUDA-library solution for accelerating trinomial pricing of options, which we are not aware of being publicly available before. Also we hope that this work will inspire new language/compiler technology, because, as the next section will show, none of the data-parallel languages can generate both efficient code versions that have been studied in this thesis.

%A common pattern can be observed - none the above mentioned methods/compilers can support the building of trinomial trees, due to the irregular parallelism that is required by it. While they are used to transform regular arrays, more complex applications such as the Hull-White One Factor Model present other difficulties if higher performance is expected. E.g. it is not possible to implement an effective multiple options per thread block implementation in Futhark, simply because of the the memory allocation constraints in the language. 

\section{Data-Parallel Language Design}
\label{relwork-lang}

The design of data-parallel languages is another research direction that has received a lot of attention in the past decade. The main idea here is (i) for the language to support a (smallish) set of operators, which have inherently parallel semantics (map, reduce, scan, etc.), and (ii) to build programs like puzzles by combining such parallel operators either horizontally (i.e., composition) or also in a nested fashion. This allows the language to statically provide not only safety guarantees---e.g., the absence of data races, but also cost-model guarantees---e.g., modeling the operational (asymptotic) complexity of the program.   Unfortunately, as of today, the parallel-language technology is not at the stage where it can efficiently and reliably execute complex real-world applications, such as the one discussed in this thesis.

For example SAC~\cite{SaCShared2005,GrelSchoIJPP06,HybType} is a data-parallel language that uses an imperative-C notation aimed at achieving widespread use, but is purely functional underneath.   It supports (i) shape polymorphism---i.e., one can write functions that operate uniformly on arrays of arbitrary rank, and (ii) a parallel construct named ``with-loop'', which can be seen as a sophisticated composition of map-reduce operators.   Unfortunately, the optimizing compiler is mainly aimed at multi-cores, rather than many-cores, architectures.

Accelerate~\cite{Accelerate-ICFP} is an array language, embedded in Haskell, aimed at GPGPU execution. It initially supported neither nested parallelism nor expression of sequential recurrences (loops) inside parallel code, but recent extensions~\cite{AccelerateStreaming} support a limited form of nested parallelism that is amenable to streaming. It follows that neither the one-option-per-thread nor the multiple-options-per-block code versions of trinomial pricing could be expressed in or derived by Accelerate. (We expect that the fully flat version can be written by hand in Accelerate, much like it has been done in Futhark.) 

The seminal work on Nesl shows that it is possible to support the expression of arbitrary-nested and irregular parallelism, by a transformation named ``flattening'', which systematically rewrites the nested parallelism into only flat-parallel operators~\cite{blelloch1994implementation}, which can be further mapped to GPGPU hardware~\cite{Bergstrom:2012:NDG:2398856.2364563}.  More importantly, this transformation guarantees that the resulted (flat-parallel) program respects the work-depth (operational) asymptotic of the original (nested-parallel) program. Unfortunately, while this transformation is theoretically appealing, it is sometimes infeasible in practice because full utilization of application parallelism prevents locality-of-reference optimizations and may require asymptotically-larger memory footprint.   It follows that Nesl would allow to elegantly express the trinomial pricing in its nested-parallel fashion, and it would automatically derive the fully-flat implementation. However, it would not be able to derive the two (more efficient) code versions, which are the subject of this thesis.

Finally, Futhark~\cite{henriksen2014bounds} is another data-parallel purely-functional language, whose design seeks a common-ground with imperative solutions. For example, it supports sequential loops with in-place modifications of array elements inside parallel regions, and an aggressive fusion mechanism~\cite{Futhark:redomap}. It also supports a ``regular'' flavor of nested parallelism---i.e., the sizes of the array dimensions produced at any program point should be invariant to the parallel-nest in which the array is defined. In this context, the flattening transformation (i) is implemented by a combination of imperative transformations~\cite{henriksen2017futhark}, such as map interchange and map fission (distribution), and (ii) it features the nice property that it can stop at an arbitrary level in the parallel nest and sequentialize from there all inner parallelism. In essence, the latter property allows further optimization of locality of reference.  Finally, Futhark allows easy integration~\cite{Henriksen:2016:AGT:2975991.2975997} of computational kernels written in it, with real-world applications written in mainstream-productivity languages such as Python and C.   In what trinomial-option pricing is concerned, Futhark supports reasonably well the code version that executes one option per thread, and it also allows manual expression of the fully-flattened program. However, Futhark cannot derive the other efficient version that processes several options in one CUDA block. We believe that this work will provide useful insights into how to engineer a restricted flavor of flattening irregular parallelism, which is carried out in fast memory and is thus efficient.   

%Multiple parallel models have also been derived from general-purpose languages to cope with more complicated applications. E.g. Accelerate\footnote{More information can be found on http://hackage.haskell.org/package/accelerate} defines an embedded array language for high-performance computing in Haskell. Computations on multi-dimensional, regular arrays are expressed in the form of parameterized collective operations, such as maps, reductions, and permutations. SAC\footnote{Read more about Single-Assignment C on http://www.sac-home.org/doku.php} is an array programming language predominantly suited for application areas such as numerically intensive applications and signal processing. Similar to Accelerate, it is also suitable only for regular arrays. Futhark is yet another a small programming language designed to be compiled to efficient parallel code. The language supports regular nested data-parallelism, as well as a form of imperative-style in-place modification of arrays \cite{henriksen2017futhark}\cite{Futhark:redomap}\cite{Henriksen:2016:AGT:2975991.2975997}.

\section{Compiler Analysis}
\label{relwork-analysis}

In this thesis we have studied how to accelerate trinomial pricing: we have identified several important performance trade-offs and have addressed each one by hand, by taking inspiration from related compiler analysis literature.
This section surveys such analyses and discusses the ones that we have used.


\subsection{Static Analysis}
\label{relwork-static}

Static analysis refers to analysis performed at compile time, which by definition does not take into account the particularities of the input datasets. Such analysis carries no runtime overhead, but it may be inaccurate (i.e., yields conservative results).   For example, analysis based on the polyhedral model~\cite{PolyhedralOpt,PolyPluto1,PolyPluto2} can reliably and aggressively optimize locality of reference and identify the parallelism of sequentially-written loops as long as the program is affine\footnote{In affine code, all array subscripts and branch conditions are expressed as affine formulas in terms of the loop-nest indices}.  This is achieved by composing a sequence of non-trivial transformations such as loop interchange, loop distribution, various kinds of loop tiling~\cite{HexaTiling}.  However, if the program is not affine (e.g., due to subscripted subscripts) then analysis is likely to fail. Analysis failing does not necessarily mean that a certain loop is not parallel, it simply means that the static analysis was not able to prove some required property. Even worse, if that property refers to a subscripted subscript, and the indirect array is part of the dataset, then it follows that no static analysis can possibly prove that property (because its proof requires information about the elements of the indirect array, which is unknown until runtime). 

Another example of static analysis is the flattening transformation~\cite{blelloch1994implementation}, which allows for increasing the amount of parallelism that can be statically mapped to hardware and which has been mentioned in the previous section. It is worth noticing that flattening is more general than the combination of loop interchange and distribution supported by Futhark---for example, Blelloch's transformation applies to parallelism which is even nested inside divide-an-conquer function calls (e.g., it is possible to flatten quicksort starting from its clearest and simplest divide-and-conquer formulation).  However, even if flattening guarantees to preserve the work-depth asymptotics of the original nested-parallel program, the analysis is still inaccurate because it fails to take into consideration communication costs or locality of reference. For example the resulted parallel (flat) operators will not contain any (sequential) recurrences/loops. While this can be good for the purpose of vectorization, it does not suit well many-core architectures and makes it impossible to optimize locality of reference.

Our implementations took inspiration from several static analyses. For example, in the one-option-per-thread implementation, we have performed loop fusion by hand and we have optimized spatial locality by working on the arrays in their transposed form~\cite{LexiFiPricing} as a way to create array accesses that are coalesced in memory---i.e., consecutive threads access consecutive memory addresses. This memory access pattern is efficiently supported by GPGPU hardware: it leads to optimal utilization of global-memory bandwidth and thus is reduces the query time of data, resulting in significant speedups.   Similarly, in the multiple-options-per-block implementation, we have taken inspiration from loop distribution (map fission) and more generally from the flattening transformation, to preform the flattening of the parallelism of several options that fit in a CUDA block. 

\subsection{Dynamic Analysis}
\label{relwork-dyn}

Dynamic analysis refers to optimizations that aggressively rely on inspecting the data at runtime. The result of this inspection is used to reorganize the data layout or the scheduling of instructions (commonly entire loop iterations) in a way that optimizes the data locality or the degree of parallelism, or the code on the hot execution path.  The runtime of the code that is performing the inspection and the reorganization constitutes pure overhead; it follows that the performed optimization need to be highly profitable in order to outset the inspection overhead. 

An illustrative example of dynamic analysis is thread-level speculation~\cite{R-LRPD,LRPD} (TLS). TLS speculatively executes loop iterations concurrently without any static guarantees that the loop is actually parallel. The inter-iterations dependencies are tracked and ``fixed'' at runtime. When such dependences are detected, the state is rolled back to a previously-known correct state, and speculation is resumed from there.   This analysis is implemented by instrumenting each read and write access to memory with code that resembles an enhanced cache-coherency protocol---in fact TLS has been successfully implemented in hardware.   On the plus side, TLS can be safely applied to automatically parallelize any application, it can exploit partial parallelism, and can be also used to optimize communication overhead in a distributed setting~\cite{jaycos:DTLS}. The shortcomings are that the overheads (both runtime and memory) are high, which makes it suboptimal in all cases and prohibitively expensive in many cases. This is because TLS has no ``smarts'', in that it does not use any static information related to the code it attempts to parallelize.   While various optimizations have been proposed, such as aimed at optimizing the footprint or the data layout of speculative memory~\cite{Oancea:2008:SDA:1485701.1485712}, TLS' overheads remain significant, which has caused TLS to loose ground in favor of techniques that combine static and dynamic analysis.  

While in the case of TLS the inspection and execution of the data and code are intermingled, most of the other instances of dynamic analysis are separating the two stages: an inspector is extracted by slicing the code of interest, and it reorganizes the data layout or the schedules of instructions, in a way that improves the execution of the original code.   For example, such analysis has been used to separate at runtime the execution of (originally) dependent loops, into waves that can be safely executed in parallel~\cite{InspExecWave}. Similarly, it has been used to optimize communication in a distributed setting~\cite{Saday:InspExec}---here the inspctor solves a graph-partitioning problem at runtime in order to determine the optimal scheduling of loop iterations to nodes.

\subsection{Hybrid Analysis (Static + Dynamic)}
\label{relwork-hyb}

Hybrid analysis attempts to find a common ground between static and dynamic analysis that combines the advantages of each, i.e., accurate analysis carrying small runtime overhead. For example, seminal work has shown how to optimize locality of reference of highly irregular computations~\cite{Kennedy:DataReord,Strout:DataItReord}, such as from the molecular-dynamics field, which make heavy use of statically-unanalyzable subscripted subscripts. This is achieved by an inspector that permutes both the array layout and the order in which loop iterations are scheduled.  The technique however is enhanced to make use of static analysis: for example it fires the transformation only when the inspector overhead can be amortized (e.g., by hoisting it outside a loop), and it rewrites the code in a manner that minimizes the number of subscripted subscripts (which requires two memory operations and puts pressure on the cache system).

Other examples of hybrid analysis are related to aggressive automatic-parallelization technique aimed at statically-unanalyzable Fortran loops. There, memory accesses are summarized at instruction, iteration and loop level under a symbolic-set representation and loop parallelism is then modeled as an equation on these sets~\cite{SUIF,HybAn}. These equations can then be solved at runtime, but the corresponding inspector has a sequential natures and it would lead to significant overhead in many cases. Instead, the equation is mapped to a language of predicates, by a logical inference analysis that extracts sufficient conditions under which the equation holds~\cite{Moon99PredArrDataFlow,SummaryMonot}. The resulting predicates have a parallel semantics, and incur negligible runtime overhead in most cases, and, in the worst case an overhead that scales down with the degree of parallelism.    

Our implementation takes inspiration from such hybrid analyses. For example, we have used inspectors that sort the options in descending order of the height/width of their trinomial tree, and we have shown that (i) this data re-ordering has a positively-high impact on performance, and that (ii) the overhead introduced by sorting is small in practice, because the sorting time is asymptotically smaller than the work necessary to price the options. Similarly, we have argued that a measure of the divergence overhead can be similarly derived by a lightweight inspector, and that a simple predicate can be autotuned to predict the most-suitable version of the code for the current dataset.


%Last, but not least, a lot of inspector/executor models have been inspirational for this thesis, used for extracting inherent parallelism from the algorithms both static and dynamic at run–time, in an attempt to optimize the locality of reference and the thread communication. This includes the ability to identify privatizable and redundant variables in order to eliminate the data dependencies\cite{InspExecWave}, run-time data and iteration-reordering transformations that focus on improving data locality\cite{Strout:DataItReord}\cite{Kennedy:DataReord} (e.g. sorting), as well as the reduction of dependencies from associative operations \cite{Saday:InspExec}(e.g. gather and scatter operations).  
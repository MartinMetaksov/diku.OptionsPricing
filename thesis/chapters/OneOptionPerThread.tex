\chapter{One Option per Thread}
\label{chapter:oneoptionperthread}
This chapter describes the first parallel approach that exploits only outer parallelism, i.e. it computes a batch of options in parallel where one thread prices a single option. 
This algorithm is therefore similar to the sequential implementation with some caveats concerning GPGPU architectures that were tackled in an iterative process producing multiple versions of the code.

\section{Sequential Implementation to CUDA}
\label{section:oneoption:seqtocuda}
\subsection*{Global memory setup}
It was necessary to identify arrays being used and consider how to store them in device memory. The input is a structure of arrays of size number of options, each array representing one parameter of the options. This structure is ideal for coalesced memory access because consecutive threads will load contiguous memory, optimizing the number of memory transactions and thus speed. The algorithm itself requires two arrays of tree width size for $\mathit{Qs}$ and $\mathit{QsCopy}$, and one array of tree height size for $\mathit{alphas}$ per option. We place the three arrays in GPU's global memory, where each thread uses a single part of each array which size depends on the option being computed.

\subsection*{Global memory accesses}
Array accesses had to be analyzed as well to avoid race conditions and optimize performance. The sequential implementation pre-computes rates and probabilities for each node along the width and saves them. They are reused later both during the forward and backward propagations. In contrast, for this CUDA implementation we would have to store the values in global memory, resulting in slower access times. Instead, since they were accessed too few times, we opted to re-compute the values every time they were used in order to save query time and reduce the global memory consumption. 

 More importantly, in the forward propagation computing $\mathit{Qs}$ at the next time step was done by a single computation on the current node and adding the value to three nodes in the next step as shown in figure~\ref{fig:seqforward}. This performs 1 read for the computation and 3 reads followed by 3 writes for the addition. For all parallel implementations, this \textbf{scatter} pattern was replaced by a \textbf{gather} pattern illustrated in fig.~\ref{fig:scattervsgather}, which performs 3 reads/computations and 1 write in this scenario.
 
 This makes it necessary to pre-compute and save the values for $\mathit{Qs}$ first to avoid computing the same value multiple times. The final result makes 1 read/computation/write in the pre-computation step followed by 3 reads and 1 write in computing the next time step. This approach eliminates 1 write and the need to have atomic additions, which would be necessary when exploiting inner parallelism described in chapter~\ref{chapter:multoptionsperthreadblock}. However, since the tree is trinomial (fig.~\ref{fig:treeconststage1}), a value can be computed from up to 5 values from the previous time step. Thus all possible types of branching have to be enumerated which makes the code more verbose and more difficult to maintain.

\begin{figure}[H]
    \centering
    \def\svgwidth{0.8\textwidth}
	\caption{Comparison of scatter and gather operations}
    \input{img/scattervsgather.pdf_tex}
	\source{Compiled by the authors}
	\label{fig:scattervsgather}
\end{figure}

\section{Implementations}
We iteratively implemented 4 versions of this first parallel approach. They all share the same kernel but they store and access the three global arrays $\mathit{Qs}$, $\mathit{QsCopy}$ and $\mathit{alphas}$ in different ways.

The general steps for all these versions are as follows:
\begin{enumerate}
    \item Load all options to GPU device memory.
    \item Compute widths and heights for all options.
    \item Allocate in global GPU memory two (expanded) arrays $\mathit{Qs}$ and $\mathit{QsCopy}$ which are large enough to hold option-width elements for all the options in the batch. What \enquote{large enough} means will be specified for each version.
    \item Similarly, allocate in global GPU memory one array $\mathit{alphas}$ which is large enough to hold option-height elements for all options in the batch.
    \item Price all options using our CUDA kernel.
    \item Copy results to host memory.
\end{enumerate}

All pre-processing is performed on the GPU and is implemented using CUDA's Thrust library~\footnote{\url{https://developer.nvidia.com/thrust}}.

\subsection*{Version 1 - Naive}
The first version stores arrays in a simple way where one thread gets contiguous parts of memory with sizes that match the computed option's width/height. Table~\ref{table:cuda-option-memory-naive} shows an example of storing $\mathit{alphas}$ for 3 options of heights 2-4-3 computed by 3 threads in a single flat array. Each thread needs to know only the start index of its array chunk along with the option's width/height and then it can access array elements consecutively. The indices can be easily computed by running inclusive scans on widths and heights, obtaining also total sizes for the arrays in the process.

\begin{table}[H]
\centering
\caption{Memory alignment in version 1}
\source{Compiled by the authors}
\label{table:cuda-option-memory-naive}
\begin{tabular}{|l|l|
>{\columncolor[HTML]{EFEFEF}}l |
>{\columncolor[HTML]{EFEFEF}}l |
>{\columncolor[HTML]{EFEFEF}}l |
>{\columncolor[HTML]{EFEFEF}}l |l|l|l|}
\hline
T1         & T1         & T2         & T2         & T2         & T2         & T3         & T3         & T3         \\ \hline
$\alpha_0$ & $\alpha_1$ & $\alpha_0$ & $\alpha_1$ & $\alpha_2$ & $\alpha_3$ & $\alpha_0$ & $\alpha_1$ & $\alpha_2$ \\ \hline
\end{tabular}
\end{table}

This approach is very efficient in terms of storage space, however, it is very inefficient when it comes to performance. When we analyze how array elements are accessed in forward (alg.~\ref{alg:sequential-forward}) and backward propagation (alg.~\ref{alg:sequential-backward}), we find out that all threads access their $\alpha_0$ at the same time, then move to $\alpha_1$ and so on. This results in strided, un-coalesced access to global memory which ineffectively uses GPU hardware. The next 3 versions tackle this problem by padding and transposing the arrays on different levels, so as to ensure coalesced accesses to global memory whenever possible.

\subsection*{Version 2 - Global-level Padding}
In order to make array access coalesced, the second version stores arrays padded to the maximum width/height across all options. Continuing with the example from above, the new alignment is illustrated in table~\ref{table:cuda-option-memory-global}. This obviously leads to some array elements not being used, unless the widths/heights are equal across options. However, when threads in a warp access the same array index at the same time, the access is now coalesced and can be performed in fewer memory transactions, greatly improving performance.

\begin{table}[H]
\centering
\caption{Memory alignment in version 2}
\source{Compiled by the authors}
\label{table:cuda-option-memory-global}
\begin{tabular}{|l|l|l|
>{\columncolor[HTML]{EFEFEF}}l |
>{\columncolor[HTML]{EFEFEF}}l |
>{\columncolor[HTML]{EFEFEF}}l |l|l|l|
>{\columncolor[HTML]{EFEFEF}}l |
>{\columncolor[HTML]{EFEFEF}}l |
>{\columncolor[HTML]{EFEFEF}}l |}
\hline
T1         & T2         & T3         & T1         & T2         & T3         & T1 & T2         & T3         & T1 & T2         & T3 \\ \hline
$\alpha_0$ & $\alpha_0$ & $\alpha_0$ & $\alpha_1$ & $\alpha_1$ & $\alpha_1$ &    & $\alpha_2$ & $\alpha_2$ &    & $\alpha_3$ &    \\ \hline
\end{tabular}
\end{table}

To compute sizes of the arrays, it is necessary just to find out the maximum width/height
and multiply them by the number of all options. Indexing to array elements can be simply computed as $\mathit{index} * \mathit{optionsCount} + \mathit{optionIndex}$.

The only downside is that the global padding might require very large memory chunks to be allocated but unused, especially if the dataset is skewed, i.e. with a small number of options that have very large widths/heights. According to our tests, discussed in detail in chapter~\ref{chapter:experimentalresults}, this version is up to \textasciitilde$10\times$ faster than version 1 but the memory footprint is up to \textasciitilde$7\times$ larger. The next two versions try to minimize the memory footprint by padding arrays on a smaller scale.

\subsection*{Version 3 - Block-level Padding}
The third version is designed to save memory compared to the second version, while keeping memory access coalesced. Here we look at what options get computed in a single CUDA block (of up to 1024 threads). The maximum width/height of the options is computed per block and the total size of an array is then the sum of all block maxima multiplied by the block size.

The pre-processing of options is thus more complex. It is implemented using $\mathit{reduce\_by\_key}$ and $\mathit{transform\_inclusive\_scan}$ Thrust routines with custom operations. As a result, arrays $\mathit{QsInds}$ and $\mathit{alphasInds}$ storing indices to the respective arrays are computed, created using two addition helper arrays, all of size \\ $\lceil \mathit{optionsCount} / \mathit{blockSize} \rceil$. The computed indices represent the start of an array part specific to a block, elements can then be accessed as $\mathit{blockSize} * \mathit{index} + \mathit{threadId}$.

The downsides of this version are that if options in the dataset are not sorted by widths and heights, the amount of saved memory compared to version 2 may be very small while this version requires more pre-processing time and intermediate arrays. With correct sorting applied and the block size kept small, on our datasets this version uses only up to 4\% more memory than version 1, while keeping up to \textasciitilde$10\times$ performance lead like version 2. The last version further reduces the array padding even on bigger block sizes.

\subsection*{Version 4 - Warp-level Padding}
The fourth version is similar to version 3 with a difference that we look at what options get computed by a single warp of 32 threads instead of the whole block.  This is motivated by the fact that coalesced accesses to global memory are supported by hardware at the (half) warp level, i.e., the threads in a warp execute in lock step, and need to access consecutive memory locations in a SIMD instruction. In effect, array padding is performed at a lower granularity---that of a warp---while still preserving coalesced accesses. Depending on a dataset, the added warp-level padding might be smaller than block-level padding, in exchange for $\mathit{blockSize} / 32$ more indices to be stored. When comparing version 4 with version 3, we achieved up to 70\% decrease in memory for a CUDA block of size 1024. 

\subsection*{Futhark implementation}
This one option per thread parallel approach was also implemented in Futhark as a proof of concept and is equivalent to version 2 since Futhark uses global padding on arrays to ensure that memory accesses are coalesced. This version will be discussed further in chapter~\ref{chapter:fullflattening}.

\subsection*{Optimizations}
\paragraph{Choosing thread block size} 
Since this implementation prices one option per thread, the thread block size represents the number of options being priced in parallel. Those options can have different widths and heights, leading to thread divergence on outer parallelism (heights) and inner parallelism (widths). It follows that bigger blocks might contain options that vary in widths/heights more than in small blocks, thus causing more threads to wait for completion of the block execution. On the other hand, smaller block sizes result in more thread blocks being scheduled, what adds extra overhead. Depending on the dataset, block sizes of either 128 or 256 are preferred.

\paragraph{Sorting input}
In order to (easily) reduce thread divergence, we can sort options before computation by either heights or widths. This would make one warp/block of threads compute options with similar number of execution steps and reduce the amount of time the threads have to wait for each other's execution. Our experiments show that sorting does have a large positive impact on performance, up to \textasciitilde$10\times$ faster than no sorting, but the choice of sorting by height or width depends on a dataset with a difference of up to 20\%.

\section{Validation}\label{section:option:validation}
To validate computed results from the CUDA implementations, we created a test case that uses the example mentioned in section~\ref{section:sequential:validation}. 100 instances of this option with gradually more time steps are first computed on the CPU using the sequential implementation from chapter~\ref{chapter:sequential} and then compared with results computed from all 4 versions. The test case was written using Catch2~\footnote{\url{https://github.com/catchorg/Catch2}} test framework for its simplicity.

The differences between floating point GPU results and CPU results must be interpreted carefully, since there are many reasons why the same sequence of operations may not be performed on the GPU and CPU, e.g. because of fused multiply-add on the GPU, rearranging operations for parallelization, higher than expected precision for computations on the CPU and rounding not required by common math operations by the IEEE 754 standard~\cite[pg. 16]{whitehead2018}.

Despite this, we successfully validated our results using a small epsilon value of std::numeric\_limits<\textbf{real}>::epsilon() $* 1000$,
where \textbf{real} is either single or double precision floating point number. This equals to $0.000119209$ for single and $0.00000000000022204$ for double precision, which makes double precision much more reliable.

\section*{Summary}
This chapter provided an overview of our parallel one option per thread implementation using CUDA with focus on explaining the challenges of parallelizing the sequential implementation from chapter~\ref{chapter:sequential}. It introduced 4 versions of this parallel implementation, each with its own advantages and disadvantages. Finally, it described how the GPU results were validated against the CPU results and the challenges of doing that. The following chapter will describe how this implementation was adapted to compute multiple options in a single CUDA thread block.
